{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------\n",
    "# Author Wessel Olaf van Dam 09-30-2019\n",
    "#\n",
    "# Machine Learning script on practice dataset \"cleveland_Heart_Disease.csv\"\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# We use the Cleveland Heart Disease dataset that has already been preprocessed, 13 features and 1 target variable\n",
    "data = pd.read_csv('cleveland_Heart_Disease.csv')\n",
    "data.head()\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# following are the 13 features used to classify heart disease or no -------\n",
    "\n",
    "# age: age in years \n",
    "# sex: sex (1 = male; 0 = female) \n",
    "# cp: chest pain type (1=typical angina, 2=atypical angina, 3=non-anginal pain, 4=asymptomatic) \n",
    "# trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n",
    "# chol: serum cholestoral in mg/dl \n",
    "# fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "#restecg: resting electrocardiographic results (0=normal,1=having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV),\n",
    "#2= showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "# thalach: maximum heart rate achieved \n",
    "# exang: exercise induced angina (1 = yes; 0 = no) \n",
    "# oldpeak = ST depression induced by exercise relative to rest \n",
    "# slope: the slope of the peak exercise ST segment (1=upsloping, 2=flat,3=downsloping) \n",
    "# ca: number of major vessels (0-3) colored by flourosopy \n",
    "# thal: 3=normal; 6=fixed defect; 7=reversable defect \n",
    "\n",
    "#The Variable to be classified\n",
    "\n",
    "#num: diagnosis of heart disease (angiographic disease status) 0: < 50% diameter narrowing, 1: > 50% diameter narrowing \n",
    "#(in any major vessel: attributes 59 through 68 are vessels)\n",
    "\n",
    "#here we recode the num variable to 0 for healthy and > 0 for heart disease\n",
    "data.loc[data.num != 0, 'num'] = 'heart disease'\n",
    "data.loc[data.num == 0, 'num'] = 'healthy'\n",
    "\n",
    "# shape\n",
    "print(data.shape)\n",
    "print(data.describe())\n",
    "print(data.groupby('num').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#We will split the loaded dataset into two, 80% of which we will use to train our models \n",
    "#and 20% that we will hold back as a validation dataset. X_train and Y_train contain the\n",
    "#training data, and X_validation and Y_validation will be used later to test the performance\n",
    "#of our models\n",
    "\n",
    "# Split-out validation dataset\n",
    "array = data.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "print (X_train.shape) # we have 297 observations altogether, now 80% is X training = 237\n",
    "print (X_validation.shape) # 20% of 297 = 60 X validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# Here we print the accuracy of each of the 6 models to see\n",
    "# which one is most accurate\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# We can see that the Gaussian Naive Bayes is the best algorithm with an accuracy of 84.4%   \n",
    "\n",
    "# plotting the results\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The NB algorithm is very simple and was an accurate model based on our tests. \n",
    "#Now we want to get an idea of the accuracy of the model on our validation set.\n",
    "\n",
    "#We can run the NB model directly on the validation set and summarize the results as a final accuracy score, \n",
    "#a confusion matrix and a classification report.\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)\n",
    "predictions = nb.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "\n",
    "# We can see that the accuracy is 0.833 or 83.3%. The confusion matrix provides \n",
    "# an indication of the 10 errors made. 28/32 healthy subjects are correctly classified\n",
    "# 22/28 heart disease patients are correctly classified.\n",
    "\n",
    "# Finally, the classification report provides a breakdown of each class by precision, recall, f1-score and \n",
    "# support showing excellent results (granted the validation dataset was small).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
