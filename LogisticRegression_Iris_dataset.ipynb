{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Part 1: importing the relevant libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Part 2: loading the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(data=iris['data'],columns=iris['feature_names'])\n",
    "df['species'] = iris['target']\n",
    "\n",
    "#df.isnull().mean() # missing values\n",
    "#df['species'].value_counts() # counts of the different species\n",
    "\n",
    "# We are going to write a binary Logistic Regression classifier,therefore we will only look\n",
    "# at class 1 and 2 from now on!\n",
    "\n",
    "# class labels right now are 1 and 2 (we will change this to 1 and 0)\n",
    "df['species'] = df['species'].replace({2:0})\n",
    "\n",
    "X = df.iloc[50:,:4].values\n",
    "y = df.iloc[50:,4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Part 3: splitting data into Training/Testing set and Feature Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Feature Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost J of theta is \n",
      "\n",
      "[51.98603854]\n"
     ]
    }
   ],
   "source": [
    "# ========== Part 4: Calculating the Cost Function J of theta\n",
    "m = len(y_train)\n",
    "X_train_padded = np.column_stack((np.ones((m,1)),X_train))\n",
    "theta = np.zeros((X_train_padded.shape[1],1))\n",
    "\n",
    "class CC(object):\n",
    "    def ComputeCost(self,X,y,theta):\n",
    "        m = len(y)\n",
    "        # calculate the Cost J of theta\n",
    "        J = 0\n",
    "        hyp_x = (1.0 / (1.0 + np.exp(-X.dot(theta))))\n",
    "        J = -y.dot(np.log(hyp_x)) - ((1-y).dot(np.log(1-hyp_x)))\n",
    "        return J\n",
    "    \n",
    "print('Initial Cost J of theta is \\n')\n",
    "print(CC().ComputeCost(X_train_padded,y_train,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gradient Descent Algorithm converged on the following theta values \n",
      "\n",
      "[[ 1.44042293]\n",
      " [ 2.32178755]\n",
      " [ 2.18429106]\n",
      " [-3.52543501]\n",
      " [-3.10671126]]\n"
     ]
    }
   ],
   "source": [
    "# ========== Part 5: Gradient Descent Algorithm to find regression coefficients that minimize cost Function J\n",
    "alpha = 0.01\n",
    "epochs = 3000\n",
    "J_history = []\n",
    "iteration_nr = []\n",
    "\n",
    "class GD(object):\n",
    "    def GradientDescent(self,alpha,epochs):\n",
    "        theta = np.zeros((X_train_padded.shape[1],1))\n",
    "        n = float(len(y_train))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            J = 0\n",
    "            hyp_x = (1.0 / (1.0 + np.exp(-X_train_padded.dot(theta))))\n",
    "            J = -y_train.dot(np.log(hyp_x)) - ((1-y_train).dot(np.log(1-hyp_x)))\n",
    "            iter = i+1\n",
    "            J_history.append(J)\n",
    "            iteration_nr.append(iter)\n",
    "            \n",
    "            #update theta values\n",
    "            gradient = (1/m) * X_train_padded.T.dot(hyp_x - np.transpose([y_train]))\n",
    "            theta = theta - alpha * gradient\n",
    "            \n",
    "        return theta\n",
    "    \n",
    "print('The Gradient Descent Algorithm converged on the following theta values \\n')\n",
    "print(GD().GradientDescent(alpha,epochs))\n",
    "final_theta = GD().GradientDescent(alpha,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logistic Regression Classifier has an accuracy of 98.67%\n"
     ]
    }
   ],
   "source": [
    "# ========== Part 6: calculating accuracy of the Logistic Regression Model on Training Data\n",
    "y_pred = (1.0 / (1.0 + np.exp(-X_train_padded.dot(final_theta))))\n",
    "y_prediction = np.zeros(len(y_pred))\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_prediction[i] = 1\n",
    "    else:\n",
    "        y_prediction[i] = 0\n",
    "\n",
    "correct = y_prediction == y_train\n",
    "accuracy = np.mean(correct)*100\n",
    "\n",
    "print('The Logistic Regression Classifier has an accuracy of', \"{:.2f}\".format(accuracy) + str(\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logistic Regression Classifier has an accuracy of 92.00%\n"
     ]
    }
   ],
   "source": [
    "# ========== Part 7: calculating the accuracy of the Logistic Regression Model on Testing Data\n",
    "m = len(y_test)\n",
    "X_test_padded = np.column_stack((np.ones((m,1)),X_test_std))\n",
    "\n",
    "y_pred = (1.0 / (1.0 + np.exp(-X_test_padded.dot(final_theta))))\n",
    "y_prediction = np.zeros(len(y_pred))\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_prediction[i] = 1\n",
    "    else:\n",
    "        y_prediction[i] = 0\n",
    "\n",
    "correct = y_prediction == y_test\n",
    "accuracy = np.mean(correct)*100\n",
    "\n",
    "print('The Logistic Regression Classifier has an accuracy of', \"{:.2f}\".format(accuracy) + str(\"%\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
